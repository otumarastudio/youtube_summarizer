import openai
import sounddevice as sd
import numpy as np
import io
import wave
from dotenv import load_dotenv
import os
import signal
from gpt_NER import extract_stock_info_gpt4o
import json

# .env íŒŒì¼ ë¡œë“œ ë° API í‚¤ ì„¤ì •
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ë…¹ìŒ ì„¤ì •
samplerate = 44100  # ìƒ˜í”Œë§ ë ˆì´íŠ¸
channels = 1        # ëª¨ë…¸ ì˜¤ë””ì˜¤
blocksize = 1024    # ì˜¤ë””ì˜¤ ì²­í¬ ì‚¬ì´ì¦ˆ

# ë²„í¼ ì„¤ì •
buffer_duration = 5  # ë²„í¼í•  ì˜¤ë””ì˜¤ ê¸¸ì´(ì´ˆ)
overlap_duration = 1  # ì˜¤ë²„ë© ê¸¸ì´(ì´ˆ)
buffer_size = int(samplerate * buffer_duration)
overlap_size = int(samplerate * overlap_duration)
audio_buffer = np.zeros((buffer_size, channels), dtype=np.float32)
buffer_index = 0

# ì „ì²´ ìŒì„± ê¸°ë¡ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
all_transcripts = []
is_running = True

def signal_handler(signum, frame):
    """Ctrl+C ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
    global is_running
    print("\nìŒì„± ì¸ì‹ì„ ì¢…ë£Œí•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
    is_running = False

def audio_callback(indata, frames, time, status):
    """ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜."""
    global buffer_index, audio_buffer
    if status:
        print(status)
    
    # ë²„í¼ì— ìƒˆ ë°ì´í„° ì¶”ê°€
    space_left = buffer_size - buffer_index
    if frames <= space_left:
        audio_buffer[buffer_index:buffer_index + frames] = indata
        buffer_index += frames
    else:
        # ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ì²˜ë¦¬
        audio_buffer[buffer_index:] = indata[:space_left]
        process_audio_chunk(audio_buffer.copy())
        
        # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ë²„í¼ì˜ ì‹œì‘ìœ¼ë¡œ ë³µì‚¬
        audio_buffer[:overlap_size] = audio_buffer[-overlap_size:]
        buffer_index = overlap_size
        
        # ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
        remaining_data = indata[space_left:]
        if len(remaining_data) > 0:
            audio_buffer[buffer_index:buffer_index + len(remaining_data)] = remaining_data
            buffer_index += len(remaining_data)

def process_audio_chunk(audio_data):
    """ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ Whisper APIë¡œ STT ë³€í™˜í•˜ê³  í…ìŠ¤íŠ¸ ì¶œë ¥."""
    pcm_audio = (audio_data * 32767).astype(np.int16)
    byte_io = io.BytesIO()

    with wave.open(byte_io, 'wb') as wf:
        wf.setnchannels(channels)
        wf.setframerate(samplerate)
        wf.setsampwidth(2)
        wf.writeframes(pcm_audio.tobytes())
    
    byte_io.seek(0)

    try:
        transcript = openai_stt(byte_io)
        if transcript.strip():
            print("ğŸ¤: " + transcript)
            all_transcripts.append(transcript)
    except Exception as e:
        print(f"STT ë³€í™˜ ì—ëŸ¬: {e}")

def openai_stt(audio_file):
    """OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜."""
    audio_file.name = "recording.wav"
    # Using a potentially better model for transcription
    transcript = openai.audio.transcriptions.create(
        model="whisper-1", 
        file=audio_file,
        language="ko"
    )
    return transcript.text

def analyze_transcripts():
    """ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ë¥¼ ê²°í•©í•˜ê³  NER ë¶„ì„ ìˆ˜í–‰"""
    if not all_transcripts:
        print("ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    full_text = " ".join(all_transcripts)
    print("\n=== ì „ì²´ ì¸ì‹ëœ í…ìŠ¤íŠ¸ ===")
    print(full_text)
    print("\n=== ì£¼ì‹ ì •ë³´ ë¶„ì„ ê²°ê³¼ ===")
    
    try:
        stock_info = extract_stock_info_gpt4o(full_text)
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            if isinstance(stock_info, str):
                parsed_info = json.loads(stock_info)
            else:
                parsed_info = stock_info

            if parsed_info and len(parsed_info) > 0:
                for info in parsed_info:
                    print(f"\nì£¼ì‹ ì¢…ëª© ì •ë³´:")
                    print(f"- ì¢…ëª©: {info.get('ì¢…ëª©', 'N/A')}")
                    print(f"- ê°€ê²©: {info.get('ê°€ê²©', 'N/A')}")
                    print(f"- ì•¡ì…˜: {info.get('ì•¡ì…˜', 'N/A')}")
                    print(f"- ì˜ê²¬: {info.get('ì˜ê²¬', 'N/A')}")
                    print(f"- ê°ì„±: {info.get('ê°ì„±', 'N/A')}")
            else:
                print("ì£¼ì‹ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ ì¶œë ¥
        except json.JSONDecodeError:
            print("GPT ë¶„ì„ ê²°ê³¼:")
            print(stock_info)
        except Exception as e:
            print("GPT ë¶„ì„ ê²°ê³¼ (êµ¬ì¡°í™” ì‹¤íŒ¨):")
            print(stock_info)
            
    except Exception as e:
        print(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def realtime_transcription():
    """ì‹¤ì‹œê°„ ìŒì„± ë…¹ìŒ ë° STT ë³€í™˜ ì‹œì‘."""
    global is_running
    
    print("ì‹¤ì‹œê°„ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œì‘... ë§ˆì´í¬ì— ëŒ€ê³  ë§í•˜ì„¸ìš”.")
    print("Ctrl+Cë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n")

    # Ctrl+C ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)

    try:
        with sd.InputStream(samplerate=samplerate,
                          channels=channels,
                          blocksize=blocksize,
                          callback=audio_callback):
            while is_running:
                sd.sleep(100)
            
            # ì¢…ë£Œ ì‹œ ë¶„ì„ ìˆ˜í–‰
            analyze_transcripts()
            
    except sd.PortAudioError as e:
        print(f"ì˜¤ë””ì˜¤ ì—ëŸ¬ ë°œìƒ: {e}")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜ ëª©ë¡:")
        print(sd.query_devices())

if __name__ == '__main__':
    realtime_transcription()
